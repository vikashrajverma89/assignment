{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7930ad-7e83-48ed-98db-d0ff11d72e4e",
   "metadata": {},
   "source": [
    "# Anomaly Detection-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227553bf-ff20-4bd4-a848-41f273c2b434",
   "metadata": {},
   "source": [
    "## Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9ec75-e47a-45e6-a079-992a1f93577b",
   "metadata": {},
   "source": [
    "Anomaly detection, also known as outlier detection, is a technique used in data analysis and machine learning to identify patterns or instances that deviate significantly from the norm or expected behavior within a dataset. The purpose of anomaly detection is to highlight unusual or rare events, observations, or patterns that may indicate potential issues, errors, or interesting insights in the data.\n",
    "\n",
    "The key objectives of anomaly detection include:\n",
    "\n",
    "1. **Identification of Unusual Patterns:** Anomaly detection helps in uncovering instances or patterns in the data that differ significantly from the majority of the observations. These anomalies may represent critical events or outliers that require further investigation.\n",
    "\n",
    "2. **Fault Detection and Prevention:** In various fields such as finance, cybersecurity, manufacturing, and healthcare, anomaly detection is used to identify faults, errors, or abnormal behavior in real-time. This allows for timely intervention and preventive measures to avoid potential issues.\n",
    "\n",
    "3. **Quality Assurance:** Anomaly detection is employed in quality control processes to identify defective products or anomalies in manufacturing processes. This ensures that only products meeting certain standards are released to the market.\n",
    "\n",
    "4. **Security:** In cybersecurity, anomaly detection is crucial for identifying unusual network behavior, potential security threats, or malicious activities. It helps in detecting anomalies that may indicate a cyberattack or unauthorized access.\n",
    "\n",
    "5. **Fraud Detection:** Anomaly detection is widely used in financial transactions to identify unusual patterns that may indicate fraudulent activities, such as credit card fraud or money laundering.\n",
    "\n",
    "6. **Health Monitoring:** In healthcare, anomaly detection can be applied to monitor patient health data, identifying unusual physiological readings that may indicate potential health issues or emergencies.\n",
    "\n",
    "7. **Predictive Maintenance:** Anomaly detection is used in industries like manufacturing and transportation to predict equipment failures or maintenance needs by identifying abnormal patterns in sensor data.\n",
    "\n",
    "Common techniques for anomaly detection include statistical methods, machine learning algorithms (such as isolation forests, one-class SVM, and autoencoders), and rule-based approaches. The choice of method depends on the nature of the data and the specific requirements of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417f61e-2b52-457f-963c-4eb52e1497a2",
   "metadata": {},
   "source": [
    "## Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0865a-aee3-4919-8dec-51c57502bf28",
   "metadata": {},
   "source": [
    "Anomaly detection comes with several challenges, and addressing these challenges is crucial for the effectiveness of anomaly detection systems. Some key challenges include:\n",
    "\n",
    "1. **Imbalanced Datasets:** In many real-world scenarios, anomalies are rare compared to normal instances. This class imbalance can lead to biased models that are more focused on normal patterns, making it difficult to identify anomalies accurately.\n",
    "\n",
    "2. **Dynamic Nature of Data:** Data distributions and patterns may change over time, especially in dynamic environments. Anomaly detection models need to adapt to these changes and update their understanding of what is considered normal or anomalous.\n",
    "\n",
    "3. **Ambiguity in Anomalies:** Anomalies are not always clear-cut and may have varying degrees of severity. Determining the threshold for what constitutes an anomaly can be challenging, and different applications may require different levels of sensitivity.\n",
    "\n",
    "4. **Labeling and Training Data:** Obtaining labeled data for training anomaly detection models can be difficult, as anomalies are often rare and may not be well-represented in the training dataset. Manual labeling of anomalies can also be subjective and time-consuming.\n",
    "\n",
    "5. **Noise in Data:** The presence of noise or irrelevant features in the data can hinder the performance of anomaly detection models. Preprocessing and feature engineering are essential to reduce noise and focus on relevant patterns.\n",
    "\n",
    "6. **Contextual Information:** Understanding the context in which anomalies occur is crucial for accurate detection. Lack of contextual information may lead to false positives or negatives, as some events that appear anomalous may be normal in a specific context.\n",
    "\n",
    "7. **Scalability:** Anomaly detection systems need to scale with the size of the data. As datasets grow larger, computational efficiency becomes a significant concern. Implementing scalable algorithms that can handle big data is essential for real-time or near-real-time applications.\n",
    "\n",
    "8. **Human-in-the-Loop Challenges:** In certain applications, involving human experts in the loop for validating anomalies or providing additional context can be challenging. Clear communication between the model and human operators is necessary to make the system effective.\n",
    "\n",
    "9. **Adversarial Attacks:** Anomaly detection systems may be vulnerable to adversarial attacks where malicious actors attempt to manipulate the data to evade detection. Ensuring the robustness of the model against such attacks is an ongoing challenge.\n",
    "\n",
    "10. **Interpretability:** Many anomaly detection algorithms, especially complex machine learning models, lack interpretability. Understanding why a certain instance is classified as anomalous is important for trust and decision-making, especially in critical applications.\n",
    "\n",
    "Addressing these challenges often requires a combination of advanced algorithms, domain knowledge, and continuous monitoring and adaptation of the anomaly detection system. Researchers and practitioners are actively working on developing more robust and adaptable approaches to overcome these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d589c-6145-4528-8764-849a89934659",
   "metadata": {},
   "source": [
    "## Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196f0f1-96b9-48b1-8f0c-0d0f770764a9",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches used in anomaly detection, and they differ primarily in the way they leverage labeled data during the training process.\n",
    "\n",
    "1. **Unsupervised Anomaly Detection:**\n",
    "   - **Training Data:** Unsupervised anomaly detection does not require labeled training data. The algorithm is exposed only to normal data during training.\n",
    "   - **Objective:** The goal is to identify patterns that are different or deviate significantly from the norm within the dataset. The algorithm learns what is considered \"normal\" without explicit information about anomalies.\n",
    "   - **Applicability:** Unsupervised methods are particularly useful when anomalies are rare, and obtaining labeled data for each type of anomaly is impractical. They are more flexible and can adapt to changing patterns in the data.\n",
    "\n",
    "   **Common Techniques:**\n",
    "   - **Statistical Methods:** Such as z-score, modified z-score, or Gaussian distribution-based methods.\n",
    "   - **Clustering Algorithms:** Detect anomalies based on the assumption that anomalies form separate clusters from normal instances.\n",
    "   - **Density-Based Methods:** Identify anomalies in low-density regions of the data.\n",
    "\n",
    "2. **Supervised Anomaly Detection:**\n",
    "   - **Training Data:** Supervised anomaly detection requires labeled training data, where both normal and anomalous instances are explicitly identified. The model learns to distinguish between the two classes during training.\n",
    "   - **Objective:** The objective is to learn a decision boundary that separates normal instances from anomalies. The model uses the labeled information to understand the characteristics of both classes.\n",
    "   - **Applicability:** Supervised methods are effective when labeled data is available and anomalies are well-defined and represent a significant portion of the dataset.\n",
    "\n",
    "   **Common Techniques:**\n",
    "   - **Classification Algorithms:** Traditional classification algorithms, such as Support Vector Machines (SVM), Decision Trees, or Neural Networks, are trained with both normal and anomalous instances.\n",
    "   - **Ensemble Methods:** Combining multiple models to improve performance in distinguishing between normal and anomalous instances.\n",
    "   - **One-Class Classification:** Learning a model based only on normal instances and considering anything deviating from this as an anomaly.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "- **Data Requirements:** Unsupervised methods work without labeled anomalies, while supervised methods require labeled data for both normal and anomalous instances.\n",
    "- **Flexibility:** Unsupervised methods are more adaptable to changing data patterns, as they do not rely on predefined anomaly labels. Supervised methods may struggle when anomalies have not been adequately represented in the labeled training data.\n",
    "- **Training Process:** Unsupervised methods focus on learning the natural structure of the data, while supervised methods explicitly learn the distinctions between normal and anomalous instances based on labeled information.\n",
    "\n",
    "The choice between unsupervised and supervised anomaly detection depends on factors such as the availability of labeled data, the nature of anomalies, and the adaptability of the method to changing patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1911ebd-a2e5-409b-ad4a-87248851c7e3",
   "metadata": {},
   "source": [
    "## Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd94aef-a185-428b-a0be-b040def37d95",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into several main types, each with its own approach to identifying unusual patterns or instances within a dataset. The main categories of anomaly detection algorithms include:\n",
    "\n",
    "1. **Statistical Methods:**\n",
    "   - **Z-Score (Standard Score):** Measures how many standard deviations a data point is from the mean. Data points with a high z-score are considered anomalies.\n",
    "   - **Modified Z-Score:** Similar to the standard z-score but may be more robust to outliers.\n",
    "   - **Distribution-Based Methods:** Assume that normal data follows a certain statistical distribution (e.g., Gaussian distribution), and deviations from this distribution indicate anomalies.\n",
    "\n",
    "2. **Proximity-Based Methods:**\n",
    "   - **k-Nearest Neighbors (k-NN):** Identifies anomalies based on the distance of a data point to its k-nearest neighbors. Outliers are often distant from their neighbors.\n",
    "   - **Local Outlier Factor (LOF):** Measures the local density deviation of a data point with respect to its neighbors. Low-density points are considered outliers.\n",
    "\n",
    "3. **Clustering-Based Methods:**\n",
    "   - **K-Means Clustering:** Detects anomalies by considering data points that do not belong to any cluster or are far from cluster centers.\n",
    "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** Identifies anomalies as points that are not part of any dense cluster.\n",
    "\n",
    "4. **Dimensionality Reduction Methods:**\n",
    "   - **Principal Component Analysis (PCA):** Projects data into a lower-dimensional space and identifies anomalies based on the reconstruction error.\n",
    "   - **Autoencoders:** Neural network-based models that learn efficient representations of data. Anomalies are detected based on reconstruction errors.\n",
    "\n",
    "5. **One-Class Classification:**\n",
    "   - **Support Vector Machines (SVM):** Trains a model on normal instances and identifies anomalies as instances lying outside a defined boundary.\n",
    "   - **Isolation Forest:** Constructs an ensemble of decision trees and isolates anomalies by requiring fewer splits to separate them from normal instances.\n",
    "\n",
    "6. **Ensemble Methods:**\n",
    "   - **Combining Models:** Ensemble methods involve combining multiple anomaly detection models to improve overall performance and robustness.\n",
    "\n",
    "7. **Density-Based Methods:**\n",
    "   - **Kernel Density Estimation (KDE):** Estimates the probability density function of the data and identifies anomalies in low-density regions.\n",
    "   - **Minimum Covariance Determinant (MCD):** Identifies anomalies by fitting a distribution to the majority of the data and detecting deviations.\n",
    "\n",
    "8. **Time Series Anomaly Detection:**\n",
    "   - **Moving Average Methods:** Detect anomalies based on deviations from the moving average of the time series.\n",
    "   - **Seasonal Decomposition of Time Series (STL):** Decomposes time series into seasonal, trend, and remainder components to identify anomalies.\n",
    "\n",
    "The choice of the anomaly detection algorithm depends on factors such as the nature of the data, the type of anomalies to be detected, the presence of labeled data, and computational efficiency requirements. Often, a combination of methods or an ensemble approach is used to enhance the overall performance of anomaly detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796d999-82c1-43a5-a063-c80db58fa94d",
   "metadata": {},
   "source": [
    "## Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360cfbf-062a-4346-9c44-7e190353feb7",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that normal instances in a dataset exhibit similar patterns and are clustered closely together, while anomalies deviate significantly from this pattern and are located farther away in the feature space. The main assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "1. **Proximity of Normal Instances:**\n",
    "   - **Assumption:** Normal instances are expected to be close to each other in the feature space.\n",
    "   - **Rationale:** In a typical dataset, normal instances share similar characteristics or patterns. By measuring the distance between data points, anomalies can be identified as those significantly distant from the majority of normal instances.\n",
    "\n",
    "2. **Global Density Estimation:**\n",
    "   - **Assumption:** The overall density of normal instances is relatively uniform across the dataset.\n",
    "   - **Rationale:** Anomalies, being rare and deviating from the norm, are expected to be located in low-density regions. Methods like k-Nearest Neighbors (k-NN) and Local Outlier Factor (LOF) assume that anomalies have a lower local density compared to their neighbors.\n",
    "\n",
    "3. **Homogeneity of Clusters:**\n",
    "   - **Assumption:** Normal instances are expected to form homogeneous clusters.\n",
    "   - **Rationale:** Clustering-based methods, like K-Means or DBSCAN, assume that anomalies do not conform to the well-defined clusters formed by normal instances. Anomalies are often treated as data points that do not belong to any cluster or are distant from cluster centers.\n",
    "\n",
    "4. **Normal Instances as Reference Points:**\n",
    "   - **Assumption:** Normal instances serve as reference points for identifying anomalies.\n",
    "   - **Rationale:** The distance from normal instances is used as a measure of anomaly, assuming that anomalies deviate significantly from the typical patterns observed in the majority of the data.\n",
    "\n",
    "5. **Limited Influence of Outliers:**\n",
    "   - **Assumption:** Outliers or anomalies have limited influence on distance measurements.\n",
    "   - **Rationale:** Outliers, being rare, are not expected to significantly affect the calculation of distances. Methods like LOF explicitly consider the local density and are less sensitive to isolated anomalies.\n",
    "\n",
    "It's important to note that while these assumptions are foundational to distance-based anomaly detection methods, they may not hold in all situations. Deviations from these assumptions can impact the performance of these methods, and users should carefully consider the characteristics of their specific datasets when choosing an anomaly detection approach. Additionally, the effectiveness of these methods can be influenced by the choice of distance metric, the dimensionality of the data, and the presence of noise or irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee0532-bde2-44d7-9efb-cc6ebf09cb7e",
   "metadata": {},
   "source": [
    "## Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4541816-daee-4f97-ac2f-cdb73eb5000e",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores for each data point in a dataset based on the local density deviation of that point with respect to its neighbors. The higher the LOF score, the more likely the data point is considered an anomaly. Here's an overview of how LOF calculates anomaly scores:\n",
    "\n",
    "1. **Local Reachability Density (LRD):**\n",
    "   - For each data point, the LRD is calculated, representing the inverse of the average local density of that point's neighbors. It is computed as the ratio of the average reachability distance of the point to its k-nearest neighbors and the reachability distance of the point itself.\n",
    "   - Mathematically, LRD for a data point \\(p\\) is given by:\n",
    "     \\[ LRD(p) = \\frac{\\text{Sum of reachability distances from } p \\text{ to its } k \\text{ neighbors}}{k \\times \\text{Reachability distance from } p \\text{ to its } k\\text{-th neighbor}} \\]\n",
    "\n",
    "2. **Local Outlier Factor (LOF) Calculation:**\n",
    "   - The LOF for each data point is then computed as the average ratio of its LRD to the LRDs of its k-nearest neighbors. LOF measures how much the local density of a point deviates from the density of its neighbors.\n",
    "   - Mathematically, LOF for a data point \\(p\\) is given by:\n",
    "     \\[ LOF(p) = \\frac{\\text{Sum of LRDs of } p\\text{'s neighbors}}{k \\times \\text{LRD}(p)} \\]\n",
    "   - Higher LOF values indicate that the point has a lower local density compared to its neighbors, suggesting that it may be an anomaly.\n",
    "\n",
    "3. **Normalization of LOF Scores:**\n",
    "   - LOF scores are often normalized to facilitate comparison across different datasets or scales. This is done by dividing each LOF score by the average LOF score of the dataset.\n",
    "     \\[ \\text{Normalized LOF}(p) = \\frac{LOF(p)}{\\text{Average LOF score of the dataset}} \\]\n",
    "\n",
    "4. **Interpretation of LOF Scores:**\n",
    "   - A high LOF score indicates that a data point has a lower local density compared to its neighbors, suggesting it is likely an anomaly. Conversely, a low LOF score suggests that the point's local density is similar to that of its neighbors, making it less likely to be an anomaly.\n",
    "\n",
    "In summary, the LOF algorithm evaluates the local density of each data point relative to its neighbors, and anomalies are identified based on the deviations in local density. The algorithm is effective in detecting anomalies that may not be globally isolated but exhibit a lower local density in their respective neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba69707-3334-47a9-b63a-5fc57d98add6",
   "metadata": {},
   "source": [
    "## Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bac53-12a1-4f44-b592-7a7907b41d38",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is an unsupervised anomaly detection algorithm that isolates anomalies by constructing decision trees. It is based on the idea that anomalies are easier to isolate than normal instances. The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "1. **Number of Trees (n_estimators):**\n",
    "   - **Description:** This parameter specifies the number of isolation trees to be created. Increasing the number of trees generally improves the performance and robustness of the algorithm, but it also comes with increased computational cost.\n",
    "   - **Default:** Common default values range from 50 to 100 trees.\n",
    "\n",
    "2. **Subsample Size (max_samples):**\n",
    "   - **Description:** The number of samples drawn from the dataset to create each isolation tree. A smaller subsample size can lead to faster training, but larger values may enhance the robustness of the model.\n",
    "   - **Default:** Common default values range from 256 to the size of the training dataset.\n",
    "\n",
    "3. **Maximum Depth of Trees (max_depth):**\n",
    "   - **Description:** The maximum depth or height of each isolation tree. Controlling the depth helps prevent overfitting and contributes to the efficiency of the algorithm.\n",
    "   - **Default:** No default maximum depth is set, and the trees are grown until each leaf contains only one instance or the specified minimum samples per leaf is reached.\n",
    "\n",
    "4. **Minimum Samples per Leaf (min_samples_leaf):**\n",
    "   - **Description:** The minimum number of samples required to create a leaf node in the isolation tree. A higher value helps prevent overfitting but may result in less fine-grained isolation.\n",
    "   - **Default:** Common default values range from 1 to 5.\n",
    "\n",
    "These parameters allow users to customize the behavior of the Isolation Forest algorithm based on the characteristics of their dataset and the desired trade-off between computational efficiency and model performance. Tuning these parameters often involves experimentation to find the optimal configuration for a specific anomaly detection task. The Isolation Forest algorithm is known for its scalability and efficiency in high-dimensional datasets, making it suitable for a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb192f-29d4-40a2-86d3-77c0d5fd7a34",
   "metadata": {},
   "source": [
    "## Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661fe09-836d-49ad-8d23-f62be22c6311",
   "metadata": {},
   "source": [
    "In the context of k-Nearest Neighbors (KNN) anomaly detection, the anomaly score for a data point is often calculated based on the distance to its k-nearest neighbors. The assumption is that normal instances will have neighbors of the same class within a certain radius, while anomalies may not.\n",
    "\n",
    "Given your scenario where a data point has only 2 neighbors of the same class within a radius of 0.5, and you want to calculate the anomaly score using KNN with \\( K = 10 \\), here's a general approach:\n",
    "\n",
    "1. **Calculate Reachability Distance:**\n",
    "   - For each neighbor within the radius, calculate the reachability distance from the data point. The reachability distance is the distance from the data point to its neighbor.\n",
    "\n",
    "2. **Select k Nearest Neighbors:**\n",
    "   - If the number of neighbors found is less than \\( k \\), you might need to consider additional neighbors to make up the required \\( k \\). This might involve expanding the search radius.\n",
    "\n",
    "3. **Calculate Anomaly Score:**\n",
    "   - The anomaly score is often based on the average or maximum reachability distance of the k-nearest neighbors. A lower average or maximum reachability distance indicates a higher likelihood of the data point being an anomaly.\n",
    "\n",
    "Without specific distance values, it's challenging to provide an exact calculation. However, you could proceed as follows:\n",
    "\n",
    "- Suppose you find 2 neighbors within the radius of 0.5.\n",
    "- Calculate the reachability distance for each of these neighbors.\n",
    "- If the number of neighbors is less than \\( k = 10 \\), you may need to expand the search radius or consider other neighbors until you reach \\( k \\).\n",
    "- Once you have the reachability distances for the k-nearest neighbors, compute the anomaly score. For example, you might take the average or maximum of these distances.\n",
    "\n",
    "Keep in mind that the exact details of the anomaly score calculation can vary based on the specific implementation or algorithm you are using for KNN-based anomaly detection. It's recommended to consult the documentation or source code of the particular implementation you're working with for precise details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab51fe59-05ea-4ec0-a4c8-77c56fcf94c3",
   "metadata": {},
   "source": [
    "## Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d8182-96ad-4c86-a399-0861536d4812",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score for a data point is often calculated based on its average path length in the isolation trees. The idea is that anomalies tend to have shorter average path lengths compared to normal instances. The average path length for a data point is the average depth of the data point across all trees in the forest.\n",
    "\n",
    "Given the information provided:\n",
    "- Number of trees (\\(n_{\\text{estimators}}\\)) = 100\n",
    "- Dataset size = 3000 data points\n",
    "- Average path length for the data point in question = 5.0\n",
    "\n",
    "The anomaly score for a data point in the Isolation Forest is typically defined as a measure of how different its average path length is compared to the expected average path length for normal instances. It's often normalized to facilitate comparison across different datasets.\n",
    "\n",
    "The formula for anomaly score (AS) is commonly defined as follows:\n",
    "\n",
    "\\[ AS = 2^{-\\frac{\\text{average path length}}{c}} \\]\n",
    "\n",
    "Here, \\(c\\) is a constant that depends on the average path length of normal instances in the dataset.\n",
    "\n",
    "Since you have the average path length for the data point (\\(5.0\\)), you can use the formula to calculate the anomaly score. However, without knowing the average path length for normal instances in the dataset or the constant \\(c\\), it's challenging to provide a specific numerical value for the anomaly score.\n",
    "\n",
    "In practice, the anomaly score is often used for ranking instances rather than providing an absolute measure. A lower anomaly score generally indicates a higher likelihood of being an anomaly. If the algorithm is implemented in a specific library or framework, you may refer to its documentation for additional details on the anomaly score calculation and any constants used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2fd7d6-fe2c-432b-8cc8-662edc1c0a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
