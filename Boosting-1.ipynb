{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c0576d-8d5f-4344-a340-8f79a7919077",
   "metadata": {},
   "source": [
    "## Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7552b9-79d4-4c73-958e-62efbf70caf4",
   "metadata": {},
   "source": [
    "Boosting is a machine learning ensemble technique that combines the predictions of multiple weak learners (typically decision trees) to create a strong learner. The primary goal of boosting is to improve the overall predictive performance of a model by reducing bias and variance.\n",
    "\n",
    "Here's a general overview of how boosting works:\n",
    "\n",
    "1. **Weak Learners:** Boosting starts with a weak learner, which is a model that performs slightly better than random chance. In many cases, decision trees with a limited depth (stumps) are used as weak learners.\n",
    "\n",
    "2. **Iterative Training:** The weak learner is trained on the entire dataset, and the algorithm assigns higher weights to the misclassified instances. In subsequent iterations, the algorithm focuses more on the misclassified instances from the previous rounds, giving them higher importance.\n",
    "\n",
    "3. **Weighted Combination:** The predictions from each weak learner are combined with weights assigned based on their accuracy. Misclassified instances receive higher weights, and the final prediction is obtained by aggregating the weighted predictions.\n",
    "\n",
    "4. **Boosted Model:** The process is repeated for a specified number of iterations or until a predefined accuracy is achieved. The final model, often referred to as the \"boosted model,\" is a combination of these weak learners, each contributing to areas where others may have performed poorly.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost. These algorithms differ in the way they assign weights, adjust for misclassifications, and combine the weak learners.\n",
    "\n",
    "Boosting is effective in improving the performance of models, especially when dealing with complex and noisy datasets. It helps in building robust models that generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a30ae-978f-4d1e-9474-383382062c14",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207c679-530c-43f8-a0aa-81b1f327f212",
   "metadata": {},
   "source": [
    "**Advantages of Boosting Techniques:**\n",
    "\n",
    "1. **Improved Accuracy:** Boosting often leads to higher accuracy compared to individual weak learners. By combining multiple models, boosting can effectively reduce bias and variance, resulting in better overall predictive performance.\n",
    "\n",
    "2. **Robustness to Overfitting:** Boosting helps to mitigate overfitting, especially when using weak learners with limited complexity. The ensemble nature of boosting tends to generalize well to new, unseen data.\n",
    "\n",
    "3. **Handling Non-Linearity:** Boosting algorithms are capable of capturing complex relationships and non-linear patterns in the data. This makes them suitable for a wide range of machine learning tasks.\n",
    "\n",
    "4. **Feature Importance:** Boosting algorithms provide a measure of feature importance, indicating the relevance of each feature in making predictions. This information can be valuable for feature selection and understanding the underlying patterns in the data.\n",
    "\n",
    "5. **Versatility:** Boosting can be applied to various types of weak learners, making it a versatile technique. It is commonly used with decision trees, but it can be adapted to other base learners as well.\n",
    "\n",
    "**Limitations of Boosting Techniques:**\n",
    "\n",
    "1. **Sensitivity to Noisy Data and Outliers:** Boosting can be sensitive to noisy data and outliers. If there are mislabeled or outlier instances in the training set, the boosting algorithm may assign too much importance to them, leading to overfitting.\n",
    "\n",
    "2. **Computational Complexity:** Training multiple weak learners sequentially can be computationally expensive and time-consuming, especially for large datasets. However, some optimized implementations like XGBoost and LightGBM have addressed this to some extent.\n",
    "\n",
    "3. **Potential for Overfitting:** While boosting helps mitigate overfitting, there is still a risk, especially if the number of weak learners is too high or if the weak learners are too complex. Careful tuning of hyperparameters is required to balance model complexity and performance.\n",
    "\n",
    "4. **Black-Box Nature:** The ensemble model created by boosting can be complex and act as a black box, making it challenging to interpret the underlying decision-making process.\n",
    "\n",
    "5. **Parameter Sensitivity:** Boosting algorithms have several hyperparameters that require careful tuning. Sensitivity to parameter choices can make it challenging to find the optimal configuration for a specific problem.\n",
    "\n",
    "In summary, while boosting techniques offer significant advantages in terms of predictive performance and robustness, practitioners should be mindful of their limitations and conduct thorough experimentation and tuning to achieve optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1b476-871f-4bb1-add5-f859bf4d61f5",
   "metadata": {},
   "source": [
    "## Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2ec7e-086d-40c5-ab95-5977b620ce66",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong learner. The basic idea behind boosting is to iteratively train a series of weak models, giving more emphasis to instances that were misclassified in previous iterations. Here's a step-by-step explanation of how boosting works:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - All data points in the training set are initially given equal weights.\n",
    "   - A weak learner (e.g., a decision tree with limited depth) is trained on the entire dataset.\n",
    "\n",
    "2. **Weighted Training:**\n",
    "   - The algorithm evaluates the performance of the weak learner.\n",
    "   - Instances that are misclassified by the weak learner are assigned higher weights for the next iteration.\n",
    "   - The next weak learner is trained on the updated dataset, giving more importance to the misclassified instances.\n",
    "\n",
    "3. **Iterative Process:**\n",
    "   - Steps 1 and 2 are repeated for a predefined number of iterations or until a certain performance criterion is met.\n",
    "   - In each iteration, a new weak learner is trained, and weights are adjusted based on the performance of the ensemble so far.\n",
    "\n",
    "4. **Combination of Weak Learners:**\n",
    "   - The predictions of all weak learners are combined with weights assigned based on their individual performance.\n",
    "   - Misclassified instances receive higher influence in the final prediction.\n",
    "   - The combined model is the boosted model, which is the weighted sum of the weak learners.\n",
    "\n",
    "The key principle is that each weak learner focuses on the mistakes of the ensemble up to that point, and by combining their predictions, the boosting algorithm aims to correct those mistakes over iterations.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost. These algorithms differ in how they assign weights, handle misclassifications, and combine the weak learners.\n",
    "\n",
    "The boosting process is effective in improving the overall accuracy of the model, reducing bias and variance, and creating a robust learner that generalizes well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1ec7a-d04d-40f3-b524-3e405fbadbfe",
   "metadata": {},
   "source": [
    "## Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccecebd-777f-483d-a22a-7aecb51a725f",
   "metadata": {},
   "source": [
    "There are several boosting algorithms, each with its own characteristics and variations. Some of the most prominent boosting algorithms include:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):**\n",
    "   - AdaBoost is one of the earliest and most well-known boosting algorithms.\n",
    "   - It assigns different weights to training instances based on their classification accuracy, focusing more on misclassified instances in subsequent iterations.\n",
    "   - Weak learners are combined with weighted majority voting to form the final model.\n",
    "\n",
    "2. **Gradient Boosting:**\n",
    "   - Gradient Boosting builds an ensemble of weak learners in a sequential manner.\n",
    "   - Each weak learner corrects the errors of the ensemble by fitting to the residuals of the predictions made by the previous learners.\n",
    "   - It minimizes a loss function, typically using gradient descent, to optimize the model.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting):**\n",
    "   - XGBoost is an efficient and scalable implementation of gradient boosting.\n",
    "   - It includes regularization terms in the objective function to control overfitting.\n",
    "   - XGBoost incorporates advanced features such as parallel processing, tree pruning, and handling missing values.\n",
    "\n",
    "4. **LightGBM (Light Gradient Boosting Machine):**\n",
    "   - LightGBM is another gradient boosting framework designed for speed and efficiency.\n",
    "   - It uses a histogram-based approach to represent feature values, enabling faster training on large datasets.\n",
    "   - LightGBM supports parallel and distributed computing.\n",
    "\n",
    "5. **CatBoost:**\n",
    "   - CatBoost is a boosting algorithm designed to handle categorical features seamlessly.\n",
    "   - It implements an efficient method for encoding categorical variables and incorporates various strategies to deal with overfitting.\n",
    "   - CatBoost aims to require minimal hyperparameter tuning.\n",
    "\n",
    "6. **Boosted Decision Trees (e.g., Stochastic Gradient Boosting):**\n",
    "   - Boosting can be applied to decision trees, and variations like Stochastic Gradient Boosting (SGD) exist.\n",
    "   - In SGD, a random subset of data is used to train each weak learner, introducing an element of stochasticity.\n",
    "\n",
    "These boosting algorithms share the common principle of combining weak learners to create a strong, robust model. However, they differ in their specific implementations, optimization techniques, and handling of various aspects such as categorical features, parallelization, and regularization. The choice of the algorithm often depends on the characteristics of the dataset and the specific requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9355a-8f9a-44db-8b53-294470e59ad1",
   "metadata": {},
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142da66-ec6f-4b38-a2c2-c161db167edf",
   "metadata": {},
   "source": [
    "Boosting algorithms have several parameters that can be tuned to optimize model performance and prevent overfitting. Here are some common parameters found in boosting algorithms:\n",
    "\n",
    "1. **Number of Weak Learners (n_estimators):**\n",
    "   - Represents the number of weak learners (trees in the case of decision tree-based algorithms) to be trained.\n",
    "   - Increasing the number of learners can improve performance up to a point, but it may also lead to longer training times and overfitting.\n",
    "\n",
    "2. **Learning Rate (or Shrinkage) (learning_rate):**\n",
    "   - Controls the contribution of each weak learner to the final prediction.\n",
    "   - Lower values require more weak learners but often result in better generalization.\n",
    "   - It helps in regularization by reducing the impact of individual weak learners.\n",
    "\n",
    "3. **Maximum Depth of Weak Learners (max_depth):**\n",
    "   - Limits the depth of individual weak learners (trees).\n",
    "   - Prevents overfitting by restricting the complexity of individual trees.\n",
    "\n",
    "4. **Subsample:**\n",
    "   - Represents the fraction of the dataset used to train each weak learner.\n",
    "   - Helps introduce randomness and reduce overfitting by training on different subsets of data.\n",
    "\n",
    "5. **Column (Feature) Subsampling:**\n",
    "   - Controls the fraction of features randomly chosen to grow each tree.\n",
    "   - Reduces the correlation between weak learners and helps prevent overfitting.\n",
    "\n",
    "6. **Minimum Child Weight (min_child_weight):**\n",
    "   - Specifies the minimum sum of instance weight (hessian) needed in a child.\n",
    "   - A higher value can lead to more conservative tree growth.\n",
    "\n",
    "7. **Gamma (min_split_loss):**\n",
    "   - Specifies the minimum loss reduction required to make a further partition on a leaf node.\n",
    "   - It helps in controlling tree complexity and preventing overfitting.\n",
    "\n",
    "8. **Regularization Parameters (lambda for L2, alpha for L1):**\n",
    "   - L2 regularization (lambda) penalizes the square of the coefficients.\n",
    "   - L1 regularization (alpha) penalizes the absolute values of the coefficients.\n",
    "   - These parameters help control overfitting by adding a penalty term to the optimization objective.\n",
    "\n",
    "9. **Scale Pos Weight (scale_pos_weight):**\n",
    "   - Addresses class imbalance by assigning different weights to positive and negative class instances.\n",
    "\n",
    "10. **Sampling Methods (e.g., Bagging Fraction in LightGBM):**\n",
    "    - Parameters related to sampling methods, such as bagging fraction in LightGBM, control the sampling strategy for building weak learners.\n",
    "\n",
    "These parameters might have different names or additional variations depending on the specific boosting algorithm being used (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost). Proper tuning of these parameters is crucial for achieving optimal model performance and avoiding overfitting. Grid search or random search techniques are often employed to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909b6f2-3c15-48a8-b6be-4784d2348fcd",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36c79c-9266-4049-9138-e0cf90b7eb0e",
   "metadata": {},
   "source": [
    "Boosting algorithms combine weak learners to create a strong learner through an iterative and weighted aggregation process. The general procedure involves assigning weights to instances in the training dataset, training a weak learner, and updating the weights based on the learner's performance. The final prediction is then formed by combining the predictions of all weak learners with appropriate weights. Here's a step-by-step explanation of how boosting algorithms typically combine weak learners:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Assign equal weights to all instances in the training dataset.\n",
    "   - Initialize the ensemble model as an empty model.\n",
    "\n",
    "2. **Iterative Training:**\n",
    "   - Train a weak learner (e.g., a decision tree with limited depth) on the weighted dataset.\n",
    "   - Evaluate the performance of the weak learner on the training set.\n",
    "   - Compute the error or residuals by comparing the weak learner's predictions to the true labels.\n",
    "\n",
    "3. **Weight Update:**\n",
    "   - Increase the weights of instances that were misclassified or had higher residuals.\n",
    "   - Decrease the weights of instances that were correctly classified or had lower residuals.\n",
    "   - This emphasizes the importance of misclassified instances in the next iteration.\n",
    "\n",
    "4. **Combine Predictions:**\n",
    "   - Assign a weight to the weak learner based on its performance, typically considering its error rate or the reduction in loss.\n",
    "   - Combine the predictions of all weak learners by summing or averaging them, with weights assigned based on their individual performance.\n",
    "   - The final prediction is formed as a weighted sum or average of the weak learners' predictions.\n",
    "\n",
    "5. **Iteration:**\n",
    "   - Repeat steps 2-4 for a specified number of iterations or until a stopping criterion is met.\n",
    "   - Each iteration focuses on correcting the mistakes of the ensemble made in previous rounds.\n",
    "\n",
    "6. **Final Model:**\n",
    "   - The boosted model is the combination of all weak learners, with weights assigned based on their individual contributions to minimizing errors.\n",
    "\n",
    "The key idea is that each weak learner specializes in the areas where the ensemble has made mistakes, and by iteratively focusing on these mistakes, the boosting algorithm constructs a strong learner. The final model tends to perform well on the training data and generalizes effectively to new, unseen data.\n",
    "\n",
    "Different boosting algorithms may implement variations of this process, but the fundamental concept of sequentially training weak learners and combining them with appropriate weights remains consistent across most boosting frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ffe7b-4d86-4fc8-8738-e9a62a3beaf1",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ab9a7-f34f-4ecb-8142-b480d0269baa",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is one of the pioneering and widely used boosting algorithms in machine learning. It was introduced by Yoav Freund and Robert Schapire in 1996. The primary goal of AdaBoost is to combine the predictions of weak learners (usually shallow decision trees) to create a strong learner that performs well on the overall dataset.\n",
    "\n",
    "Here's an overview of how the AdaBoost algorithm works:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Assign equal weights to all training instances, making the initial weights uniform.\n",
    "   - Initialize an empty model to represent the ensemble.\n",
    "\n",
    "2. **Iterative Training:**\n",
    "   - In each iteration, a weak learner (e.g., a decision tree with limited depth) is trained on the weighted dataset.\n",
    "   - The weak learner aims to minimize the weighted error, where the weights emphasize the importance of misclassified instances from the previous iteration.\n",
    "   - The error of the weak learner is calculated as the sum of the weights of misclassified instances.\n",
    "\n",
    "3. **Compute Weak Learner's Weight:**\n",
    "   - Calculate the weight of the weak learner based on its error rate. A lower error rate results in a higher weight.\n",
    "   - The weight is used to determine the importance of the weak learner's prediction in the final ensemble.\n",
    "\n",
    "4. **Update Instance Weights:**\n",
    "   - Increase the weights of instances that were misclassified by the weak learner, making them more influential in the next iteration.\n",
    "   - Decrease the weights of correctly classified instances, making them less influential in subsequent iterations.\n",
    "\n",
    "5. **Combine Predictions:**\n",
    "   - Combine the predictions of all weak learners by assigning weights based on their individual performance.\n",
    "   - The final prediction is formed as a weighted sum of the weak learners' predictions.\n",
    "\n",
    "6. **Iteration:**\n",
    "   - Repeat steps 2-5 for a specified number of iterations or until a stopping criterion is met.\n",
    "   - Each iteration focuses on correcting the mistakes of the ensemble made in previous rounds.\n",
    "\n",
    "7. **Final Model:**\n",
    "   - The final AdaBoost model is the weighted combination of all weak learners, with higher weights assigned to those that performed well on the training data.\n",
    "\n",
    "The strength of AdaBoost lies in its adaptability to focus on instances that are challenging to classify. It is particularly effective in handling imbalanced datasets and noisy data. However, AdaBoost can be sensitive to outliers and noisy data, and care should be taken to preprocess the data appropriately.\n",
    "\n",
    "In summary, AdaBoost is a powerful boosting algorithm that builds a strong model by iteratively combining weak learners, giving more emphasis to instances that are difficult to classify correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007dda1-a3da-4118-85e1-715f6a78e397",
   "metadata": {},
   "source": [
    "## Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912eb286-b4ff-4090-b732-dea45cf65859",
   "metadata": {},
   "source": [
    "AdaBoost uses an exponential loss function, also known as the AdaBoost loss function or the exponential loss. The exponential loss function is chosen because it is well-suited for boosting algorithms, such as AdaBoost, where the emphasis is on correctly classifying instances and assigning higher weights to misclassified instances.\n",
    "\n",
    "The goal of AdaBoost is to minimize the weighted sum of exponential losses across all instances in the training dataset. In each iteration, AdaBoost focuses on training a weak learner that reduces the overall exponential loss by giving higher weights to misclassified instances. The weights assigned to instances are updated in each iteration to emphasize the importance of instances that were misclassified in the previous rounds.\n",
    "\n",
    "The use of the exponential loss function in AdaBoost contributes to the algorithm's ability to adapt to difficult-to-classify instances and improve overall performance through the iterative training of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8257bf2-7460-41ec-b0b4-c836dd1a569a",
   "metadata": {},
   "source": [
    "## Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde20e0-a172-46dc-be09-3b90eb31e6eb",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm updates the weights of misclassified samples in each iteration to give more emphasis to the instances that were difficult to classify correctly. The idea is to penalize misclassifications more heavily in the subsequent rounds, guiding the algorithm to focus on the mistakes made by the ensemble. The weight update process involves the following steps:\n",
    "\n",
    "Compute the Error of the Weak Learner:\n",
    "\n",
    "    Train a weak learner on the weighted dataset.\n",
    "    \n",
    "Calculate the Weight of the Weak Learner:\n",
    "\n",
    "    Calculate the weight of the weak learner based on its error rate.\n",
    "    \n",
    "Update Instance Weights:\n",
    "\n",
    "    Increase the weights of misclassified instances and decrease the weights of correctly classified instances.\n",
    "    \n",
    "Normalize Weights:\n",
    "\n",
    "    Normalize the updated weights to ensure that they sum up to 1.\n",
    "    \n",
    "    \n",
    "    Normalization is necessary to maintain the property that weights sum to 1, making them valid probability distributions.\n",
    "\n",
    "These steps are repeated for each iteration of AdaBoost. The effect of this weight updating process is that instances that are consistently misclassified receive higher weights, guiding the subsequent weak learners to focus more on these challenging instances. The final prediction is then formed by combining the predictions of all weak learners with weights based on their individual performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bbc38-1b96-4a4a-baea-5394eb33cb24",
   "metadata": {},
   "source": [
    "## Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16703ece-d59c-48eb-aca0-42549b366ee2",
   "metadata": {},
   "source": [
    "Increasing the number of estimators (weak learners or trees) in the AdaBoost algorithm can have both positive and negative effects on the model's performance. Here are the key effects:\n",
    "\n",
    "**Positive Effects:**\n",
    "\n",
    "1. **Improved Training Performance:**\n",
    "   - As the number of estimators increases, the model has more opportunities to learn from the training data and correct its mistakes.\n",
    "   - This often leads to improved training performance, reducing bias and potentially achieving a more accurate and complex model.\n",
    "\n",
    "2. **Better Generalization:**\n",
    "   - AdaBoost aims to reduce both bias and variance, and increasing the number of estimators can contribute to better generalization.\n",
    "   - The ensemble becomes more robust and capable of capturing complex relationships in the data.\n",
    "\n",
    "3. **Increased Model Capacity:**\n",
    "   - A larger number of estimators increases the overall model capacity, allowing the algorithm to capture finer details and patterns in the training data.\n",
    "   - This can be beneficial when dealing with complex datasets.\n",
    "\n",
    "4. **Reduction in Underfitting:**\n",
    "   - With more estimators, AdaBoost becomes less prone to underfitting as the model has more opportunities to adapt to the intricacies of the data.\n",
    "\n",
    "**Negative Effects:**\n",
    "\n",
    "1. **Overfitting Risk:**\n",
    "   - While AdaBoost is designed to reduce overfitting, increasing the number of estimators beyond a certain point may lead to overfitting, especially if the weak learners are too complex.\n",
    "   - The model may start fitting the noise in the training data rather than learning true patterns.\n",
    "\n",
    "2. **Computational Complexity:**\n",
    "   - Training a larger number of estimators increases the computational complexity and time required for training.\n",
    "   - The algorithm needs to fit more weak learners sequentially, and this may become a limiting factor, especially for large datasets.\n",
    "\n",
    "3. **Diminishing Returns:**\n",
    "   - There may be diminishing returns regarding performance improvement beyond a certain number of estimators.\n",
    "   - After a certain point, the additional weak learners may contribute less to overall performance improvement, and the computational cost may outweigh the benefits.\n",
    "\n",
    "**Recommendations:**\n",
    "- It's advisable to monitor the model's performance on a validation set and possibly conduct cross-validation to find the optimal number of estimators.\n",
    "- Regularization techniques such as limiting the depth of weak learners or adjusting learning rates can be employed to mitigate overfitting.\n",
    "- Practical considerations such as computational resources and time constraints should be taken into account when deciding on the number of estimators.\n",
    "\n",
    "In summary, increasing the number of estimators in AdaBoost can lead to improved training performance and generalization, but it's crucial to strike a balance to avoid overfitting and consider practical constraints. Cross-validation and monitoring performance on validation sets are valuable practices when tuning hyperparameters like the number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669da77-a8dc-4eae-a3e0-4fba5b548ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
