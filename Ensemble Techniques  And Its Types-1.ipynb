{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d777b762-aece-48d0-a11c-9ecd6ba046f3",
   "metadata": {},
   "source": [
    "## Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b1dad-5719-421d-a4aa-b510eedbba24",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning is a methodology that combines the predictions of multiple base models to create a more robust and accurate model. The idea behind ensemble methods is to leverage the strengths of various individual models to improve overall performance and generalize well to new, unseen data. Ensemble techniques are widely used across different machine learning tasks, including classification, regression, and clustering.\n",
    "\n",
    "There are several types of ensemble methods, with two main categories being:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):**\n",
    "   - **Random Forest:** It is a popular bagging technique that builds multiple decision trees during training and combines their predictions through a voting mechanism.\n",
    "   - **Bagged Decision Trees:** Building multiple decision trees using bootstrap samples of the training data and averaging their predictions.\n",
    "\n",
    "2. **Boosting:**\n",
    "   - **AdaBoost (Adaptive Boosting):** Boosting technique that assigns weights to data points and iteratively trains weak models, giving more weight to misclassified samples in each iteration.\n",
    "   - **Gradient Boosting:** Builds trees sequentially, with each tree correcting the errors made by the previous ones. Examples include XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "Ensemble methods can improve model performance by reducing overfitting, increasing model stability, and capturing complex relationships in the data. The diversity among the base models is crucial for the success of ensemble techniques; if the individual models are too similar, the benefits of ensemble learning may be limited.\n",
    "\n",
    "Ensemble methods are known for their effectiveness in a wide range of applications and are often considered a go-to strategy to enhance model performance in various machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29bf0dc-9d60-4c3e-ae61-513f6bb6056f",
   "metadata": {},
   "source": [
    "## Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76aaa73-fd23-4825-ad6c-ba6552f0fbea",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons, and they offer various advantages that contribute to improved model performance and robustness. Here are some key reasons why ensemble techniques are widely employed:\n",
    "\n",
    "1. **Improved Accuracy:**\n",
    "   - Ensemble methods often lead to more accurate predictions compared to individual base models. By combining the strengths of multiple models, ensembles can reduce bias and variance, resulting in better overall accuracy.\n",
    "\n",
    "2. **Reduction of Overfitting:**\n",
    "   - Overfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. Ensemble techniques, particularly bagging methods, help reduce overfitting by combining multiple models trained on different subsets of the data, leading to a more robust model.\n",
    "\n",
    "3. **Increased Robustness:**\n",
    "   - Ensembles enhance the robustness of models, making them less sensitive to noise or outliers in the data. If a particular base model is influenced by noise or outliers, the impact is minimized when combined with other models.\n",
    "\n",
    "4. **Handling Model Complexity:**\n",
    "   - Ensemble methods can effectively handle complex relationships in the data. Boosting algorithms, in particular, build models sequentially, with each subsequent model correcting errors made by the previous ones. This sequential learning process allows ensembles to capture intricate patterns in the data.\n",
    "\n",
    "5. **Versatility Across Algorithms:**\n",
    "   - Ensemble techniques are algorithm-agnostic, meaning they can be applied to a variety of base models, such as decision trees, support vector machines, or neural networks. This versatility allows practitioners to leverage the strengths of different algorithms within a unified framework.\n",
    "\n",
    "6. **Increased Stability:**\n",
    "   - Ensembles provide increased stability to model predictions. If individual models are sensitive to variations in the training data, ensembles help mitigate this sensitivity, resulting in more consistent and reliable predictions.\n",
    "\n",
    "7. **Flexibility and Adaptability:**\n",
    "   - Ensemble methods can be applied to different machine learning tasks, including classification, regression, and clustering. They can adapt to various types of data and model structures, making them versatile in different problem domains.\n",
    "\n",
    "8. **Model Interpretability:**\n",
    "   - While individual models may be complex and challenging to interpret, the ensemble's overall predictions can often be more interpretable. Techniques like feature importance in tree-based ensembles provide insights into which features are crucial for making predictions.\n",
    "\n",
    "Ensemble techniques, such as Random Forests, AdaBoost, and Gradient Boosting, have demonstrated their effectiveness in numerous real-world applications, making them a popular choice in machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e162af-0ee9-421d-8eb7-754c702500a7",
   "metadata": {},
   "source": [
    "## Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266d6a4-701f-4d0e-8bc0-13c7cce1b85f",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble machine learning technique that aims to improve the overall performance and robustness of models by training multiple instances of the same base model on different subsets of the training data. The fundamental idea behind bagging is to reduce overfitting and variance by introducing diversity among the base models.\n",
    "\n",
    "Here's a step-by-step explanation of the bagging process:\n",
    "\n",
    "1. **Bootstrap Sampling:**\n",
    "   - Bagging involves creating multiple random subsets (samples) of the training dataset through a process called bootstrap sampling.\n",
    "   - Bootstrap sampling involves randomly selecting samples from the original dataset with replacement. Some instances may appear multiple times, while others may not be included in a particular subset.\n",
    "\n",
    "2. **Training Base Models:**\n",
    "   - For each bootstrap sample, a base model is trained independently on that specific subset of the data.\n",
    "   - The base model can be any algorithm capable of making predictions, such as decision trees, which are commonly used in bagging.\n",
    "\n",
    "3. **Aggregation of Predictions:**\n",
    "   - Once all the base models are trained, their predictions are aggregated to make a final prediction.\n",
    "   - In the case of regression problems, predictions are often averaged. For classification problems, the majority vote or averaging of class probabilities is used.\n",
    "\n",
    "The key advantages of bagging include:\n",
    "\n",
    "- **Variance Reduction:** By training on multiple subsets of the data, bagging helps reduce the variability in predictions that might result from training on a single dataset.\n",
    "\n",
    "- **Overfitting Reduction:** Bagging can mitigate overfitting, especially when base models are sensitive to noise in the training data.\n",
    "\n",
    "- **Increased Robustness:** The combination of predictions from diverse models tends to produce more reliable and robust results, less influenced by outliers or peculiarities in the training data.\n",
    "\n",
    "The most well-known bagging algorithm is the **Random Forest**, which employs bagging with decision trees as the base models. Random Forests extend the bagging technique by introducing additional randomness during the training of individual trees, such as feature subsampling. This further enhances the diversity among the base models and contributes to improved overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0934a2-dd02-4a02-9de8-cf39af287b1c",
   "metadata": {},
   "source": [
    "## Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbeb3c1-97e7-4baf-bc63-766f54d0fbdd",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique that aims to improve the performance of weak learners (models that are slightly better than random guessing) by combining their predictions in a weighted manner. Unlike bagging, which builds independent models in parallel, boosting builds a sequence of models, with each subsequent model focusing on correcting the errors made by the previous ones. The main idea behind boosting is to create a strong learner by iteratively emphasizing the instances that are misclassified or poorly predicted by the existing models.\n",
    "\n",
    "Here's a step-by-step explanation of the boosting process:\n",
    "\n",
    "1. **Base Model Training:**\n",
    "   - The first base model is trained on the entire training dataset.\n",
    "   - The model might not perform well, but it serves as a starting point.\n",
    "\n",
    "2. **Instance Weighting:**\n",
    "   - Assign weights to each instance in the training dataset. Initially, all instances have equal weights.\n",
    "\n",
    "3. **Model Iteration:**\n",
    "   - For each iteration:\n",
    "     - Train a new base model on the training data with instance weights.\n",
    "     - Adjust the instance weights to give more importance to misclassified instances from the previous iteration.\n",
    "     - Combine the predictions of all models with weighted averaging or voting.\n",
    "\n",
    "4. **Iteration Termination:**\n",
    "   - Repeat the iteration process until a predefined number of models are built or until the model reaches satisfactory performance.\n",
    "\n",
    "The most common boosting algorithms include:\n",
    "\n",
    "- **AdaBoost (Adaptive Boosting):** Assigns higher weights to misclassified instances, allowing subsequent models to focus on correcting these mistakes.\n",
    "\n",
    "- **Gradient Boosting:** Builds models sequentially, with each model minimizing the residual errors of the combined ensemble.\n",
    "\n",
    "- **XGBoost, LightGBM, and CatBoost:** Advanced boosting frameworks that optimize the boosting process, offering efficient parallelization, regularization, and handling of missing values.\n",
    "\n",
    "Key advantages of boosting include:\n",
    "\n",
    "- **Improved Accuracy:** Boosting aims to reduce bias and variance, resulting in a more accurate predictive model.\n",
    "\n",
    "- **Model Adaptation:** Boosting algorithms adapt to the complexities of the data, capturing intricate patterns and relationships.\n",
    "\n",
    "- **Handling of Weak Models:** Boosting can boost the performance of weak learners, making them collectively strong.\n",
    "\n",
    "- **Robustness:** Boosting can make the model more robust to noise and outliers in the data.\n",
    "\n",
    "- **Feature Importance:** Many boosting algorithms provide insights into feature importance, helping interpret the model.\n",
    "\n",
    "Boosting is a powerful technique used in various machine learning applications, and its effectiveness has led to the development of several boosting algorithms that are widely applied in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec54c9b-88c1-4ce9-a8ae-79a99eced2e7",
   "metadata": {},
   "source": [
    "## Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a7023-2765-4abe-b9f3-9e1fd99c2f2c",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in machine learning, contributing to improved model performance, robustness, and generalization. Here are some key benefits of using ensemble techniques:\n",
    "\n",
    "1. **Increased Accuracy:**\n",
    "   - Ensemble methods often result in more accurate predictions compared to individual base models. By combining multiple models with diverse perspectives, ensembles can mitigate errors and biases, leading to improved overall accuracy.\n",
    "\n",
    "2. **Reduction of Overfitting:**\n",
    "   - Ensemble techniques, especially bagging methods, help reduce overfitting by introducing diversity among the base models. Overfitting occurs when a model learns the training data too well, capturing noise and outliers. Ensembles smooth out these variations, leading to better generalization to new data.\n",
    "\n",
    "3. **Improved Robustness:**\n",
    "   - Ensembles increase the robustness of models, making them less sensitive to noise, outliers, or anomalies in the data. The combination of predictions from diverse models helps create a more stable and reliable overall model.\n",
    "\n",
    "4. **Enhanced Generalization:**\n",
    "   - Ensemble methods are effective at capturing complex relationships in the data and generalizing well to unseen instances. This makes ensembles suitable for a wide range of applications and datasets.\n",
    "\n",
    "5. **Versatility Across Algorithms:**\n",
    "   - Ensemble techniques are algorithm-agnostic, meaning they can be applied to different base models, including decision trees, support vector machines, neural networks, and more. This versatility allows practitioners to leverage the strengths of various algorithms within a unified framework.\n",
    "\n",
    "6. **Handling of Imbalanced Data:**\n",
    "   - Ensembles can handle imbalanced datasets effectively. In classification problems where one class is underrepresented, ensemble methods balance the influence of each class, preventing the model from being biased towards the majority class.\n",
    "\n",
    "7. **Flexibility and Adaptability:**\n",
    "   - Ensemble methods can be applied to different machine learning tasks, including classification, regression, and clustering. They adapt well to various types of data and model structures, making them versatile in different problem domains.\n",
    "\n",
    "8. **Interpretability:**\n",
    "   - While individual models may be complex and challenging to interpret, ensembles can provide insights into the overall decision-making process. Techniques such as feature importance in tree-based ensembles offer valuable information about which features contribute most to predictions.\n",
    "\n",
    "9. **Handling of Nonlinear Relationships:**\n",
    "   - Ensemble methods, especially boosting algorithms, are effective at capturing nonlinear relationships in the data. This makes them suitable for tasks where the underlying patterns are intricate and may not be well-captured by a single model.\n",
    "\n",
    "10. **Reduction of Variance:**\n",
    "    - Ensembles, particularly bagging methods, help reduce variance by combining the predictions of multiple models. This reduction in variance contributes to more stable and reliable model predictions.\n",
    "\n",
    "By leveraging the benefits of ensemble techniques, practitioners can build models that are not only accurate but also more robust and adaptable to various challenges in real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91c579-6544-4e1c-ab61-8a1f21f45e2c",
   "metadata": {},
   "source": [
    "## Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036d36f-176d-4e59-926e-702dcf9b57d3",
   "metadata": {},
   "source": [
    "While ensemble techniques often lead to improved model performance, they are not always guaranteed to be better than individual models. The effectiveness of ensemble techniques depends on several factors, and there are situations where individual models might perform equally well or even outperform ensembles. Here are some considerations:\n",
    "\n",
    "1. **Diversity Among Base Models:**\n",
    "   - The success of ensemble techniques relies on the diversity among the base models. If the individual models are too similar or if they make similar errors, the ensemble may not provide significant improvements. Ensuring diversity is crucial for the success of ensemble methods.\n",
    "\n",
    "2. **Quality of Base Models:**\n",
    "   - If the base models used in the ensemble are weak or poorly trained, combining them might not yield better results. Ensemble techniques are most effective when built on top of competent base models that capture different aspects of the data.\n",
    "\n",
    "3. **Size and Complexity of the Dataset:**\n",
    "   - For small and simple datasets, individual models may perform well, and the benefits of ensembles might be marginal. Ensembles tend to shine in large and complex datasets where diverse models can collectively capture intricate patterns.\n",
    "\n",
    "4. **Computational Resources:**\n",
    "   - Ensemble methods, especially boosting algorithms, can be computationally expensive and time-consuming. In situations where computational resources are limited, the trade-off between model performance and computational cost needs to be considered.\n",
    "\n",
    "5. **Model Interpretability:**\n",
    "   - If interpretability is a crucial requirement, individual models might be preferred over ensembles. Ensemble methods, particularly those involving multiple base models, can be more challenging to interpret.\n",
    "\n",
    "6. **Nature of the Problem:**\n",
    "   - The type of machine learning problem plays a role. For certain problems, such as straightforward classification tasks with well-separated classes, a single well-tuned model might be sufficient.\n",
    "\n",
    "7. **Ensemble Type and Hyperparameters:**\n",
    "   - The choice of ensemble type (bagging or boosting) and the hyperparameters used during training can significantly impact performance. It is essential to experiment with different configurations to find the optimal settings for a specific problem.\n",
    "\n",
    "8. **Ensemble Size:**\n",
    "   - The number of models in the ensemble can influence performance. In some cases, a small ensemble might be sufficient, while in others, larger ensembles might be required to achieve the desired results.\n",
    "\n",
    "9. **Time and Resource Constraints:**\n",
    "   - In scenarios where there are strict time or resource constraints, training and maintaining an ensemble might not be practical. In such cases, a well-tuned individual model might be preferred.\n",
    "\n",
    "10. **Overfitting Considerations:**\n",
    "    - While ensemble methods can help mitigate overfitting, there are instances where they might inadvertently introduce overfitting, especially if not carefully controlled. Balancing the bias-variance trade-off is essential.\n",
    "\n",
    "In summary, while ensemble techniques are powerful tools for improving model performance, their superiority depends on the specific characteristics of the data, the nature of the problem, and the quality of the base models. It is advisable to experiment with both individual models and ensembles, considering the specific context and requirements of the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c1be0-d490-4d76-bad9-7aee9f13ce00",
   "metadata": {},
   "source": [
    "## Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9e2ef-eb25-40c1-a0b4-f9d7fabc3008",
   "metadata": {},
   "source": [
    "In statistics, a confidence interval provides a range of values that is likely to contain the true value of an unknown parameter. Bootstrap is a resampling technique that can be used to estimate the variability of a sample statistic and, consequently, calculate confidence intervals. The process involves creating multiple bootstrap samples from the original dataset, computing the sample statistic of interest for each sample, and then using the distribution of these statistics to construct the confidence interval.\n",
    "\n",
    "Here's a step-by-step guide on how to calculate a confidence interval using bootstrap:\n",
    "\n",
    "Collect the Original Data:\n",
    "\n",
    "Begin with your original dataset, which contains the observations for which you want to estimate a parameter or calculate a statistic.\n",
    "Resampling (Bootstrap Sampling):\n",
    "\n",
    "Generate multiple bootstrap samples by randomly selecting observations with replacement from the original dataset. Each bootstrap sample should have the same size as the original dataset.\n",
    "Calculate the Statistic of Interest:\n",
    "\n",
    "For each bootstrap sample, compute the sample statistic of interest (e.g., mean, median, standard deviation, etc.). This could be the parameter you want to estimate or any other statistic you are interested in.\n",
    "Create a Distribution of Sample Statistics:\n",
    "\n",
    "Collect the computed sample statistics from all the bootstrap samples to create a distribution. This distribution represents the variability of the sample statistic.\n",
    "Calculate Confidence Interval:\n",
    "\n",
    "Determine the desired confidence level for your interval (common choices are 90%, 95%, or 99%).\n",
    "Find the corresponding percentiles of the distribution. For example, for a 95% confidence interval, you would typically use the 2.5th percentile as the lower bound and the 97.5th percentile as the upper bound.\n",
    "Construct the Confidence Interval:\n",
    "\n",
    "Use the calculated percentiles to define the lower and upper bounds of the confidence interval. The interval should contain the sample statistic values within the specified confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65d99b8-4717-4ef1-90e2-d09d19b2d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95%): [3.7 7.2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 1000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_samples = [np.random.choice(original_data, len(original_data), replace=True) for _ in range(num_samples)]\n",
    "\n",
    "# Calculate sample means for each bootstrap sample\n",
    "bootstrap_sample_means = [np.mean(sample) for sample in bootstrap_samples]\n",
    "\n",
    "# Calculate percentiles for confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval (95%):\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab94d46-e5a2-4472-acae-9e11c9e93106",
   "metadata": {},
   "source": [
    "## Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb874a6f-012a-4d0e-af7b-8b3606f74003",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics to estimate the variability of a sample statistic by repeatedly resampling with replacement from the observed data. The main idea is to create multiple datasets, called bootstrap samples, by drawing observations from the original dataset, allowing for the estimation of the sampling distribution of a statistic. Here are the general steps involved in bootstrap:\n",
    "\n",
    "Collect the Original Data:\n",
    "\n",
    "Begin with the observed dataset, containing the actual observations from the population.\n",
    "Random Sampling with Replacement:\n",
    "\n",
    "Generate multiple bootstrap samples by randomly selecting observations from the original dataset with replacement. Each bootstrap sample has the same size as the original dataset.\n",
    "Calculate the Statistic of Interest:\n",
    "\n",
    "For each bootstrap sample, compute the sample statistic of interest. This could be the mean, median, standard deviation, correlation, or any other statistic you want to estimate or analyze.\n",
    "Repeat the Process:\n",
    "\n",
    "Repeat steps 2 and 3 a large number of times (typically thousands or more) to create a distribution of the sample statistic. Each iteration represents a possible realization of the sampling variability.\n",
    "Analyze the Distribution:\n",
    "\n",
    "Examine the distribution of the sample statistic obtained from the bootstrap resampling. This distribution provides insights into the variability of the sample statistic and can be used for constructing confidence intervals or assessing the uncertainty associated with the estimate.\n",
    "Compute Confidence Intervals:\n",
    "\n",
    "Use the distribution of the sample statistic to calculate confidence intervals. The percentiles of the distribution can be used to define the lower and upper bounds of the interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f57712-2fa9-4915-b145-f42ff2b463ce",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282291a0-f39b-4bda-89b8-8dcebe1fbcd6",
   "metadata": {},
   "source": [
    "Collect the Original Data:\n",
    "\n",
    "The researcher has measured the height of a sample of 50 trees. The sample mean height is 15 meters, and the sample standard deviation is 2 meters.\n",
    "Bootstrap Resampling:\n",
    "\n",
    "Generate multiple bootstrap samples by randomly selecting observations from the original sample with replacement. Each bootstrap sample should have the same size as the original sample (50 trees).\n",
    "Calculate Sample Means for Bootstrap Samples:\n",
    "\n",
    "For each bootstrap sample, calculate the sample mean height.\n",
    "Construct Confidence Interval:\n",
    "\n",
    "Use the distribution of sample means obtained from the bootstrap samples to construct the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd667ab-23cb-4baf-9d70-16021e0cf885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95%): [15. 15.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample data\n",
    "original_sample = np.array([15] * 50)  # Using the mean value for simplicity\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_samples = [np.random.choice(original_sample, len(original_sample), replace=True) for _ in range(num_samples)]\n",
    "\n",
    "# Calculate sample means for each bootstrap sample\n",
    "bootstrap_sample_means = [np.mean(sample) for sample in bootstrap_samples]\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap Confidence Interval (95%):\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117d8b1-afc9-4ac3-b74c-a9708b9ffbc2",
   "metadata": {},
   "source": [
    "In this example, I used a simplified case where the original sample consists of 50 trees with a mean height of 15 meters. The code generates 10,000 bootstrap samples, calculates the sample mean for each bootstrap sample, and then computes the 95% confidence interval using percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40b27d-4726-47ad-812f-f2770a19add8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
