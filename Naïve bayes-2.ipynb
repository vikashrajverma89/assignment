{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4938cec-dfb4-49fa-bd0d-0b1b8cc7bf64",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2f014-3dbb-42f5-8844-7b700e11ffb1",
   "metadata": {},
   "source": [
    "we use conditional probability to find the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Let's denote:\n",
    "S: The event that an employee is a smoker.\n",
    "H: The event that an employee uses the health insurance plan.\n",
    "\n",
    "The probability that an employee uses the health insurance plan is P(H)=0.70, and the probability that an employee is a smoker given that he/she uses the health insurance plan is P(S∣H)=0.40.\n",
    "\n",
    "You can calculate the probability that an employee is a smoker given that he/she uses the health insurance plan (P(S∣H)) using the conditional probability formula:\n",
    "P(S∣H)= P(S∩H) / P(H)\n",
    "where \n",
    "\n",
    "P(S∩H) is the probability that an employee is both a smoker and uses the health insurance plan. Rearrange the formula to solve for P(S∩H)\n",
    "P(S∩H)=P(S∣H)⋅P(H)\n",
    "Now, substitute the given values\n",
    "P(S∩H)=0.40⋅0.70\n",
    "P(S∩H)=0.28\n",
    "\n",
    "So, the probability that an employee is both a smoker and uses the health insurance plan \n",
    "P(S∩H)) is 0.28."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd7350-2623-4092-a0ce-bfc4a2b1c0e8",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff44fa-07cf-4b97-a388-b6d41edfa4db",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the types of features they are designed to handle and the underlying assumptions they make about the data.\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Features:** Bernoulli Naive Bayes is suitable for binary data, where features are either present or absent (0 or 1).\n",
    "   - **Application:** It is commonly used in text classification tasks, where each feature represents the presence or absence of a word in a document.\n",
    "   - **Assumption:** Assumes that features are conditionally independent given the class label.\n",
    "   - **Probability Model:** Utilizes the Bernoulli distribution to model the likelihood of features.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Features:** Multinomial Naive Bayes is designed for discrete data, often used with count data, where features represent counts of occurrences (e.g., word frequencies in a document).\n",
    "   - **Application:** Widely used in text classification, spam filtering, and other tasks where features are counts of occurrences.\n",
    "   - **Assumption:** Assumes that features are conditionally independent given the class label.\n",
    "   - **Probability Model:** Utilizes the Multinomial distribution to model the likelihood of features.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the data and the representation of features. If the features are binary (present/absent), Bernoulli Naive Bayes is more appropriate. If the features are counts or frequencies, Multinomial Naive Bayes is a better choice. Both algorithms share the \"naive\" assumption that features are conditionally independent, which simplifies the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91b61b-772e-43d7-b0b7-8fa6106e1214",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a7457-d820-4b5a-aac0-383b958bb137",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes classifiers, is based on the assumption that features are conditionally independent given the class label. When it comes to missing values, the handling typically depends on how the missing values are represented or imputed in the dataset.\n",
    "\n",
    "Here are a few common approaches:\n",
    "\n",
    "1. **Imputation with a placeholder value:**\n",
    "   - If the missing values are explicitly represented in the dataset, you can impute them with a specific placeholder value (e.g., 0 or 1) to indicate the absence or presence of the feature. This way, the missing values are treated as if they have a known value.\n",
    "\n",
    "2. **Imputation with the feature's mode:**\n",
    "   - Another approach is to impute missing values with the mode (the most frequent value) of the feature. This assumes that the most common value is a reasonable estimate for the missing values. However, this approach may not be suitable if the feature has a balanced distribution.\n",
    "\n",
    "3. **Use a separate category for missing values:**\n",
    "   - Instead of imputing missing values with specific values, you can treat missing values as a separate category. This allows the classifier to consider the absence of information as a distinct category during the classification process.\n",
    "\n",
    "The choice of how to handle missing values depends on the nature of the data and the characteristics of the missing values. It's important to note that Naive Bayes classifiers, including Bernoulli Naive Bayes, are generally robust to missing data, especially if the missing values are handled appropriately during the preprocessing stage. However, the effectiveness of the chosen approach depends on the specific characteristics of the dataset and the impact of missing values on the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2432e3-df8d-4d80-ba64-f4794cae5b77",
   "metadata": {},
   "source": [
    "In Python, handling missing values for Bernoulli Naive Bayes involves preprocessing your dataset before applying the classifier. Here is a simple example using the scikit-learn library:\n",
    "\n",
    "Let's assume you have a dataset with missing values represented as NaN and you want to impute them with a placeholder value, such as 0 or 1.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with features X1, X2, ..., Xn and a target column 'target'\n",
    "# Replace NaN values with a placeholder value, e.g., 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Bernoulli Naive Bayes classifier\n",
    "classifier = BernoulliNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5327551-fbc5-4bb7-a576-09c12132de47",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d594a-c7be-42e7-9a6b-6f221c8f38ab",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that is suitable for continuous data, and it can be applied to problems with more than two classes. The extension to multiple classes is often referred to as Gaussian Naive Bayes for multi-class classification.\n",
    "\n",
    "In the multi-class setting, Gaussian Naive Bayes uses the Gaussian distribution to model the likelihood of features within each class. The key assumption is that the features within each class are normally distributed.\n",
    "\n",
    "Here's a basic outline of how you can use Gaussian Naive Bayes for multi-class classification in Python using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9ac3ca-a9d9-4d54-af40-f087d9091a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset (an example multi-class dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28b62d-255b-46d0-b26a-82a884f4d9a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2772a6f-aae1-40d4-82b5-e903cecee779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
