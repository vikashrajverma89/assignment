{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6191625c-8a2f-44ae-9a65-590d01873867",
   "metadata": {},
   "source": [
    "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10803b5e-28c0-4626-9c07-f6b10658647a",
   "metadata": {},
   "source": [
    "In machine learning, kernel functions play a crucial role in transforming input data into higher-dimensional spaces, enabling algorithms to find complex patterns that might not be evident in the original feature space. Polynomial functions are a specific type of kernel function.\n",
    "\n",
    "The relationship between polynomial functions and kernel functions is that polynomial kernels are a type of kernel function used in support vector machines (SVMs) and other machine learning algorithms. A kernel function calculates the dot product of the transformed input data in a higher-dimensional space without explicitly computing the transformation. Polynomial kernels achieve this by using polynomial functions to create new features.\n",
    "The general form of a polynomial kernel is given by:\n",
    "K(Xi,Xj)=(Xi**T*Xj+C)**d\n",
    "where:\n",
    "Xi and Xj are input data points.\n",
    "c is a constant term.\n",
    "d is the degree of the polynomial.\n",
    "\n",
    "This kernel function essentially captures interactions between features up to a certain degree d without explicitly transforming the data into a higher-dimensional space. The dot product in the transformed space is efficiently computed, allowing algorithms like SVMs to work in higher-dimensional feature spaces without explicitly calculating the transformed data points.\n",
    "\n",
    "In summary, polynomial kernels are a specific type of kernel function that uses polynomial functions to implicitly map input data into a higher-dimensional space, facilitating the discovery of complex relationships in the data by machine learning algorithms. Kernel functions, in general, provide a powerful and flexible way to incorporate non-linearity into models while avoiding the computational cost of explicitly working in higher-dimensional spaces.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fe182-2810-4d33-a104-6a4991b1f994",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3bfbe6a-5890-4262-a7d5-a6eb3e3aae38",
   "metadata": {},
   "source": [
    "\n",
    "Implementing an SVM with a polynomial kernel in Python using Scikit-learn is relatively straightforward. Scikit-learn provides the SVC (Support Vector Classification) class that allows you to specify a polynomial kernel by setting the kernel parameter to 'poly'. Additionally, you can control the degree of the polynomial using the degree parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e63770-a2ca-438b-95c2-5c64d85a3f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "# Set degree to control the degree of the polynomial\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1.0)  # You can adjust the degree as needed\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4360e694-a309-4941-8e37-c1452d7ff527",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "We load the Iris dataset using datasets.load_iris().\n",
    "Split the dataset into a training set and a testing set using train_test_split.\n",
    "Create an SVM classifier with a polynomial kernel by setting kernel='poly' and specifying the degree using the degree parameter. You can adjust the degree parameter based on your requirements.\n",
    "Fit the model to the training data using the fit method.\n",
    "Make predictions on the test set using the predict method.\n",
    "Calculate and print the accuracy of the model on the test set using accuracy_score.\n",
    "Adjust the hyperparameters such as degree and C (regularization parameter) based on your specific problem and dataset characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4003ac-27b3-47bd-b31c-6ab633bd31df",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd39cd-abbf-4c66-a611-f83df4877af9",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon  is a crucial parameter that defines the width of the tube around the regression line within which errors are tolerated. SVR aims to fit the data within this tube while minimizing the error. The tube is determined by the EPSILON -insensitive loss function.\n",
    "\n",
    "Increasing the value of EPSILON in SVR generally leads to a wider tube, which allows more data points to fall within the margin of tolerance. Consequently, a wider tube is more permissive of errors, and the algorithm may accept a larger number of points as support vectors.\n",
    "\n",
    "Support vectors are data points that have a non-zero coefficient in the solution, meaning they contribute to determining the position of the regression line or hyperplane. In SVR, these are the points that fall either on the margin or within the tube. When EPSILON is increased, the margin widens, and more data points may become eligible as support vectors.\n",
    "\n",
    "Here's a summary of the relationship:\n",
    "\n",
    "- **Smaller ( epsilon \\):** A smaller tube is more strict, allowing fewer points to be considered support vectors. The model might try to fit the data more closely, potentially leading to overfitting.\n",
    "\n",
    "- **Larger ( epsilon \\):** A larger tube is more permissive, and more data points might fall within the margin. This may result in a model that is more robust to noise in the training data but might have a broader generalization error.\n",
    "\n",
    "It's essential to strike a balance when selecting the value of ( epsilon ) in SVR. The optimal value depends on the specific characteristics of your data and the desired trade-off between fitting the training data precisely and generalizing well to unseen data. Cross-validation or other model evaluation techniques can help in choosing an appropriate ( epsilon ) value based on the performance of the model on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15008cf5-df7a-473b-b86a-e8a9f4ea9447",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15eb6f-81e5-45b7-be06-09b5df2be3bc",
   "metadata": {},
   "source": [
    "\n",
    "Support Vector Regression (SVR) has several hyperparameters that significantly influence its performance. Here's an explanation of the key parameters and how they impact the SVR model:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "Explanation: The kernel function determines the type of mapping applied to the input features. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "Impact on Performance:\n",
    "Linear Kernel: Suitable for linear relationships between features. It's computationally efficient but may not capture complex patterns.\n",
    "Polynomial Kernel: Introduces non-linearity with a specified degree. Higher degrees can capture more complex relationships but may lead to overfitting.\n",
    "RBF Kernel: Suitable for non-linear relationships. It has two parameters: \n",
    "�\n",
    "γ (controls the shape) and C (controls the trade-off between smoothness and fitting the data).\n",
    "Sigmoid Kernel: Represents hyperbolic tangent, useful for neural networks. Similar to the RBF, it has parameters \n",
    "�\n",
    "γ and C.\n",
    "C Parameter:\n",
    "\n",
    "Explanation: The regularization parameter C controls the trade-off between fitting the training data well and having a smooth decision boundary. A smaller C encourages a smoother decision boundary, while a larger C allows the model to fit the training data more closely.\n",
    "Impact on Performance:\n",
    "Smaller C: Emphasizes smoothness, which might be desirable for avoiding overfitting. It allows more training errors.\n",
    "Larger C: Focuses on fitting the training data more precisely, potentially leading to overfitting. It penalizes training errors more heavily.\n",
    "Epsilon Parameter (ε):\n",
    "\n",
    "Explanation: Epsilon defines the margin of tolerance in the \n",
    "ε-insensitive loss function. It controls the tube around the regression line within which errors are acceptable.\n",
    "Impact on Performance:\n",
    "Smaller \n",
    "ε: Creates a narrow tolerance tube, making the model less tolerant to errors. It may lead to overfitting.\n",
    "Larger \n",
    "ε: Enlarges the tolerance tube, allowing more points to be within the margin. It increases model robustness but may reduce precision.\n",
    "Gamma Parameter:\n",
    "\n",
    "Explanation: For RBF and Polynomial kernels, \n",
    "γ controls the shape of the decision boundary. High γ alues lead to a more complex, wiggly decision boundary, while low \n",
    "γ values result in a smoother boundary.\n",
    "Impact on Performance:\n",
    "\n",
    "Smaller \n",
    "γ: Results in a broader decision boundary, suitable for capturing global patterns. It prevents overfitting but may oversimplify.\n",
    "\n",
    "Larger \n",
    "γ: Creates a more complex, localized decision boundary, fitting the training data closely. It can lead to overfitting and may not generalize well.\n",
    "\n",
    "\n",
    "- xamples:\n",
    "\n",
    "- iF your data has a clear linear relationship, start with a linear kernel.\n",
    "- For non-linear relationships, experiment with RBF or polynomial kernels.\n",
    "- If you observe overfitting, reduce C and γ or increase ε.\n",
    "- If the model is too smooth and underfitting, increase C and γ or decrease ε.\n",
    "- Use cross-validation to fine-tune hyperparameters for optimal performance on unseen data.\n",
    "The optimal parameter values depend on the specific characteristics of your data, and experimentation is often necessary to find the best combination for your particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e96052-1ad8-45c8-a4a2-5460f6caec62",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataseT\n",
    "- Split the dataset into training and testing setZ\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "- Create an instance of the SVC classifier and train it on the training datW\n",
    "- hse the trained classifier to predict the labels of the testing datW\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-scoreK\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performanc_\n",
    "- Train the tuned classifier on the entire dataseg\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bdeea40-24f0-4132-b81d-99cefeaaaa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svm_classifier.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC(kernel='rbf')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:\\n', classification_report_str)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print('Best Hyperparameters:', best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "\n",
    "tuned_classifier = grid_search.best_estimator_\n",
    "tuned_classifier.fit(X_train_scaled, y_train)  # Corrected variable name\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_classifier, 'tuned_svm_classifier.joblib')\n",
    "\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_classifier, 'tuned_svm_classifier.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaec254-2495-4e0c-95e4-9b4788c57adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
